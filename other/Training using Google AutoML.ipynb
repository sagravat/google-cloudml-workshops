{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Parameters\n",
    "\n",
    "In GCP, work is organized into projects.\n",
    "\n",
    "**gcs_path** is the path that images and the training csv will be uploaded to. AutoML Vision requires the images to be stored in a directory with the root directory being gs://{project_id}-vcm. \n",
    "\n",
    "**train_filename** is the name of the csv file that's uploaded to GCS. Doesn't matter what you choose here. \n",
    "\n",
    "**gcp_service_account_json** is the path to the service account key. Service accounts allow you to authenticate with GCP using a JSON key (rather than typing in a password). I uploaded my service account key as a private dataset. Read more about setting one up at [https://cloud.google.com/iam/docs/understanding-service-accounts](https://cloud.google.com/iam/docs/understanding-service-accounts)\n",
    "\n",
    "**train_budget** is the number of node hours. 1 is min. 24 is maximum. I believe AutoML Vision Classifaction costs $20 per node hour.\n",
    "\n",
    "**dataset_name** is the name of the dataset that's loaded into AutoML. Doesn't matter what you choose here. \n",
    "\n",
    "**model_name** is the name of the model in AutoML. Doesn't matter what you choose here. I use the convention of dataset_name *underscore* train_budget\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcp_project_id = 'agravat-dev'\n",
    "gcs_path = \"gs://{}-vcm/recursion-cellular-image-classification/RGB224/\".format(gcp_project_id)\n",
    "train_filename = \"automl_train.csv\"\n",
    "gcp_service_account_json = 'gcp_svc.json'\n",
    "gcp_compute_region = 'us-central1' #for now, AutoML is only available in this region\n",
    "train_budget = 24\n",
    "dataset_name = 'recursion_224px_wo_controls'\n",
    "model_name = \"{}_{}\".format(dataset_name,train_budget) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list | grep auto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Google Cloud SDK and AutoML Package\n",
    "Followed the instructions at [https://cloud.google.com/sdk/install](https://cloud.google.com/sdk/install) with some slight modifications for this environment. Need the Google Cloud SDK to use gsutil, which is the fast way to transfer the training data to Google Cloud Storage (GCS). \n",
    "\n",
    "Also need to install the AutoML Python Package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#google cloud SDK\n",
    "!echo \"deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main\" | tee -a /etc/apt/sources.list.d/google-cloud-sdk.list\n",
    "!apt-get install apt-transport-https ca-certificates\n",
    "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key --keyring /usr/share/keyrings/cloud.google.gpg add -\n",
    "!apt-get update && apt-get install --yes --allow-unauthenticated google-cloud-sdk\n",
    "\n",
    "#AutoML package\n",
    "!pip install google-cloud-automl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#authenticate the Google Cloud SDK\n",
    "# !gcloud config set project $gcp_project_id\n",
    "# !gcloud auth activate-service-account --key-file=$gcp_service_account_json\n",
    "\n",
    "#uncomment if you don't already have this gcs bucket setup\n",
    "!gsutil mb -p $gcp_project_id -c regional -l $gcp_compute_region gs://$gcp_project_id-vcm/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries\n",
    "Importing libraries after all packages have been installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import zipfile\n",
    "import os\n",
    "from google.cloud import automl_v1beta1 as automl\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the training CSV and upload to Google Cloud Storage (GCS)\n",
    "Here are the docs for what your data should look like for AutoML vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data by experiment\n",
    "By default, splitting betweeen train, validation and test is optional for AutoML (by default it splits randomly). But it's important for this use case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('./new_train.csv')\n",
    "\n",
    "validation = ['HEPG2-07','HUVEC-16','RPE-07','U2OS-02']\n",
    "test = ['HEPG2-03','HUVEC-04','RPE-05','U2OS-01']\n",
    "\n",
    "\n",
    "df_train['split'] = 'TRAIN' \n",
    "#put experiments in the lists aboe in Validation and TEST\n",
    "df_train.loc[df_train['experiment'].isin(validation),'split'] = 'VALIDATION' \n",
    "df_train.loc[df_train['experiment'].isin(test),'split'] = 'TEST'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload training CSV to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://automl_train.csv [Content-Type=text/csv]...\n",
      "/ [1 files][  7.5 MiB/  7.5 MiB]                                                \n",
      "Operation completed over 1 objects/7.5 MiB.                                      \n"
     ]
    }
   ],
   "source": [
    "\n",
    "#add gcs path\n",
    "df_train['gcspath'] = gcs_path + 'train/' + df_train['filename']\n",
    "\n",
    "#AutoML requires the label to be an int not a float\n",
    "df_train['sirna'] = df_train['sirna'].astype(int)\n",
    "\n",
    "df_train[['split','gcspath','sirna']].to_csv(train_filename,index=False,header=False)\n",
    "!gsutil cp $train_filename $gcs_path$train_filename #upload csv file to GCS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract images and upload to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile('./input/train.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('./train/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil -q -m cp -r ./train/* $gcs_path/ #upload images to gcs\n",
    "!rm -r ./train/ #need to do this because otherwise you get a \"too many output files error\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kick off an AutoML training job\n",
    "\n",
    "Requires a three step process:\n",
    "1. Setup an AutoML data object\n",
    "2. Load data into the object\n",
    "3. Train the model\n",
    "\n",
    "This is mostly boilerplate copied from:\n",
    "[https://cloud.google.com/vision/automl/docs/tutorial](http://https://cloud.google.com/vision/automl/docs/tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Setup AutoML Data Object\n",
    "client = automl.AutoMlClient.from_service_account_json(gcp_service_account_json)\n",
    "project_location = client.location_path(gcp_project_id, gcp_compute_region)\n",
    "\n",
    "my_dataset = {\n",
    "    \"display_name\": dataset_name,\n",
    "    \"image_classification_dataset_metadata\": {\"classification_type\": \"MULTICLASS\"},\n",
    "}\n",
    "\n",
    "# Create a dataset with the dataset metadata in the region\n",
    "dataset = client.create_dataset(project_location, my_dataset)\n",
    "dataset_id = (dataset.name.split(\"/\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing import...\n",
      "Data imported. \n"
     ]
    }
   ],
   "source": [
    "#2 Load data into the object\n",
    "dataset_full_id = client.dataset_path(\n",
    "    gcp_project_id, gcp_compute_region, dataset_id\n",
    ")\n",
    "\n",
    "input_uris = ('{}{}'.format(gcs_path ,train_filename)).split(\",\")\n",
    "input_config = {\"gcs_source\": {\"input_uris\": input_uris}}\n",
    "\n",
    "response = client.import_data(dataset_full_id, input_config)\n",
    "\n",
    "print(\"Processing import...\")\n",
    "print(\"Data imported. {}\".format(response.result()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training operation name: projects/548552332917/locations/us-central1/operations/ICN3036886850045214720\n",
      "Training started...\n"
     ]
    }
   ],
   "source": [
    "#3. Train the model\n",
    "my_model = {\n",
    "    \"display_name\": model_name,\n",
    "    \"dataset_id\": dataset_id,\n",
    "    \"image_classification_model_metadata\": {\"train_budget\": train_budget}\n",
    "    if train_budget\n",
    "    else {},\n",
    "}\n",
    "\n",
    "response = client.create_model(project_location, my_model)\n",
    "\n",
    "print(\"Training operation name: {}\".format(response.operation.name))\n",
    "print(\"Training started...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
