{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install necessary packages if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "F2uKbu7zGLnW",
    "outputId": "2bd27fa3-e566-4bc5-d447-a6d0c3c3d272"
   },
   "source": [
    "!pip install apache-beam\n",
    "!pip install tensorflow_transform\n",
    "!pip install --upgrade google-api-python-client "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "colab_type": "code",
    "id": "6TKRrIois11c",
    "outputId": "eef665be-fa9b-43cf-e613-8227f6ea0104"
   },
   "outputs": [],
   "source": [
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\n",
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DVT5D_BMZjRV"
   },
   "outputs": [],
   "source": [
    "!rm -rf census_model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "iV8vcRsKmTkN",
    "outputId": "2f3d4041-fad6-4e74-ab5a-fdaf520eacb4"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "48gEDke_Femr"
   },
   "outputs": [],
   "source": [
    "# Copyright 2017 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"Example using census data from UCI repository.\"\"\"\n",
    "\n",
    "# pylint: disable=g-bad-import-order\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import pprint\n",
    "import tempfile\n",
    "\n",
    "# GOOGLE-INITIALIZATION\n",
    "\n",
    "import apache_beam as beam\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "import tensorflow_transform.beam as tft_beam\n",
    "from tensorflow_transform.tf_metadata import dataset_metadata\n",
    "from tensorflow_transform.tf_metadata import schema_utils\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "\n",
    "\n",
    "CATEGORICAL_FEATURE_KEYS = [\n",
    "    'workclass',\n",
    "    'education',\n",
    "    'marital_status',\n",
    "    'occupation',\n",
    "    'relationship',\n",
    "    'race',\n",
    "    'sex',\n",
    "    'native_country',\n",
    "]\n",
    "NUMERIC_FEATURE_KEYS = [\n",
    "    'age',\n",
    "    'capital_gain',\n",
    "    'capital_loss',\n",
    "    'hours_per_week',\n",
    "    'education_num',\n",
    "]\n",
    "OPTIONAL_NUMERIC_FEATURE_KEYS = [\n",
    "#    'education-num',\n",
    "]\n",
    "LABEL_KEY = 'label'\n",
    "\n",
    "\n",
    "class MapAndFilterErrors(beam.PTransform):\n",
    "  \"\"\"Like beam.Map but filters out erros in the map_fn.\"\"\"\n",
    "\n",
    "  class _MapAndFilterErrorsDoFn(beam.DoFn):\n",
    "    \"\"\"Count the bad examples using a beam metric.\"\"\"\n",
    "\n",
    "    def __init__(self, fn):\n",
    "      self._fn = fn\n",
    "      # Create a counter to measure number of bad elements.\n",
    "      self._bad_elements_counter = beam.metrics.Metrics.counter(\n",
    "          'census_example', 'bad_elements')\n",
    "\n",
    "    def process(self, element):\n",
    "      try:\n",
    "        yield self._fn(element)\n",
    "      except Exception:  # pylint: disable=broad-except\n",
    "        # Catch any exception the above call.\n",
    "        self._bad_elements_counter.inc(1)\n",
    "\n",
    "  def __init__(self, fn):\n",
    "    self._fn = fn\n",
    "\n",
    "  def expand(self, pcoll):\n",
    "    return pcoll | beam.ParDo(self._MapAndFilterErrorsDoFn(self._fn))\n",
    "\n",
    "\n",
    "RAW_DATA_FEATURE_SPEC_PH = dict([(name, tf.compat.v1.placeholder(shape=[None], dtype=tf.string))\n",
    "                              for name in CATEGORICAL_FEATURE_KEYS] +\n",
    "                             [(name, tf.compat.v1.placeholder(shape=[None], dtype=tf.float32))\n",
    "                              for name in NUMERIC_FEATURE_KEYS] +\n",
    "                             [(name, tf.compat.v1.placeholder(shape=[None], dtype=tf.float32))\n",
    "                              for name in OPTIONAL_NUMERIC_FEATURE_KEYS] +\n",
    "                             [(LABEL_KEY,\n",
    "                               tf.compat.v1.placeholder(shape=[None], dtype=tf.string))])\n",
    "\n",
    "RAW_DATA_METADATA = dataset_metadata.DatasetMetadata(\n",
    "    schema_utils.schema_from_feature_spec(RAW_DATA_FEATURE_SPEC))\n",
    "\n",
    "# Constants used for training.  Note that the number of instances will be\n",
    "# computed by tf.Transform in future versions, in which case it can be read from\n",
    "# the metadata.  Similarly BUCKET_SIZES will not be needed as this information\n",
    "# will be stored in the metadata for each of the columns.  The bucket size\n",
    "# includes all listed categories in the dataset description as well as one extra\n",
    "# for \"?\" which represents unknown.\n",
    "TRAIN_BATCH_SIZE = 128\n",
    "TRAIN_NUM_EPOCHS = 1\n",
    "NUM_TRAIN_INSTANCES = 32561\n",
    "NUM_TEST_INSTANCES = 16281\n",
    "\n",
    "# Names of temp files\n",
    "TRANSFORMED_TRAIN_DATA_FILEBASE = 'train_transformed'\n",
    "TRANSFORMED_TEST_DATA_FILEBASE = 'test_transformed'\n",
    "EXPORTED_MODEL_DIR = 'exported_model_dir'\n",
    "\n",
    "# Functions for preprocessing\n",
    "\n",
    "\n",
    "def transform_data(train_data_file, test_data_file, working_dir):\n",
    "  \"\"\"Transform the data and write out as a TFRecord of Example protos.\n",
    "\n",
    "  Read in the data using the CSV reader, and transform it using a\n",
    "  preprocessing pipeline that scales numeric data and converts categorical data\n",
    "  from strings to int64 values indices, by creating a vocabulary for each\n",
    "  category.\n",
    "\n",
    "  Args:\n",
    "    train_data_file: File containing training data\n",
    "    test_data_file: File containing test data\n",
    "    working_dir: Directory to write transformed data and metadata to\n",
    "  \"\"\"\n",
    "\n",
    "  def preprocessing_fn(inputs):\n",
    "    \"\"\"Preprocess input columns into transformed columns.\"\"\"\n",
    "    # Since we are modifying some features and leaving others unchanged, we\n",
    "    # start by setting `outputs` to a copy of `inputs.\n",
    "    outputs = inputs.copy()\n",
    "\n",
    "    # Scale numeric columns to have range [0, 1].\n",
    "    for key in NUMERIC_FEATURE_KEYS:\n",
    "      outputs[key] = tft.scale_to_0_1(outputs[key])\n",
    "\n",
    "    for key in OPTIONAL_NUMERIC_FEATURE_KEYS:\n",
    "      # This is a SparseTensor because it is optional. Here we fill in a default\n",
    "      # value when it is missing.\n",
    "      sparse = tf.sparse.SparseTensor(outputs[key].indices, outputs[key].values,\n",
    "                                      [outputs[key].dense_shape[0], 1])\n",
    "      dense = tf.sparse.to_dense(sp_input=sparse, default_value=0.)\n",
    "      # Reshaping from a batch of vectors of size 1 to a batch to scalars.\n",
    "      dense = tf.squeeze(dense, axis=1)\n",
    "      outputs[key] = tft.scale_to_0_1(dense)\n",
    "\n",
    "    # For all categorical columns except the label column, we generate a\n",
    "    # vocabulary but do not modify the feature.  This vocabulary is instead\n",
    "    # used in the trainer, by means of a feature column, to convert the feature\n",
    "    # from a string to an integer id.\n",
    "    for key in CATEGORICAL_FEATURE_KEYS:\n",
    "      tft.vocabulary(inputs[key], vocab_filename=key)\n",
    "\n",
    "    # For the label column we provide the mapping from string to index.\n",
    "    table_keys = ['>50K', '<=50K']\n",
    "    initializer = tf.lookup.KeyValueTensorInitializer(\n",
    "        keys=table_keys,\n",
    "        values=tf.cast(tf.range(len(table_keys)), tf.int64),\n",
    "        key_dtype=tf.string,\n",
    "        value_dtype=tf.int64)\n",
    "    table = tf.lookup.StaticHashTable(initializer, default_value=-1)\n",
    "    outputs[LABEL_KEY] = table.lookup(outputs[LABEL_KEY])\n",
    "\n",
    "    return outputs\n",
    "\n",
    "  # The \"with\" block will create a pipeline, and run that pipeline at the exit\n",
    "  # of the block.\n",
    "  with beam.Pipeline() as pipeline:\n",
    "    with tft_beam.Context(temp_dir=tempfile.mkdtemp()):\n",
    "      # Create a coder to read the census data with the schema.  To do this we\n",
    "      # need to list all columns in order since the schema doesn't specify the\n",
    "      # order of columns in the csv.\n",
    "      ordered_columns = [\n",
    "          'age', 'workclass', 'fnlwgt', 'education', 'education_num',\n",
    "          'marital_status', 'occupation', 'relationship', 'race', 'sex',\n",
    "          'capital_gain', 'capital_loss', 'hours_per_week', 'native_country',\n",
    "          'label'\n",
    "      ]\n",
    "      converter = tft.coders.CsvCoder(ordered_columns, RAW_DATA_METADATA.schema)\n",
    "\n",
    "      # Read in raw data and convert using CSV converter.  Note that we apply\n",
    "      # some Beam transformations here, which will not be encoded in the TF\n",
    "      # graph since we don't do the from within tf.Transform's methods\n",
    "      # (AnalyzeDataset, TransformDataset etc.).  These transformations are just\n",
    "      # to get data into a format that the CSV converter can read, in particular\n",
    "      # removing spaces after commas.\n",
    "      #\n",
    "      # We use MapAndFilterErrors instead of Map to filter out decode errors in\n",
    "      # convert.decode which should only occur for the trailing blank line.\n",
    "      raw_data = (\n",
    "          pipeline\n",
    "          | 'ReadTrainData' >> beam.io.ReadFromText(train_data_file)\n",
    "          | 'FixCommasTrainData' >> beam.Map(\n",
    "              lambda line: line.replace(', ', ','))\n",
    "          | 'DecodeTrainData' >> MapAndFilterErrors(converter.decode))\n",
    "\n",
    "      # Combine data and schema into a dataset tuple.  Note that we already used\n",
    "      # the schema to read the CSV data, but we also need it to interpret\n",
    "      # raw_data.\n",
    "      raw_dataset = (raw_data, RAW_DATA_METADATA)\n",
    "      transformed_dataset, transform_fn = (\n",
    "          raw_dataset | tft_beam.AnalyzeAndTransformDataset(preprocessing_fn))\n",
    "      transformed_data, transformed_metadata = transformed_dataset\n",
    "      transformed_data_coder = tft.coders.ExampleProtoCoder(\n",
    "          transformed_metadata.schema)\n",
    "\n",
    "      _ = (\n",
    "          transformed_data\n",
    "          | 'EncodeTrainData' >> beam.Map(transformed_data_coder.encode)\n",
    "          | 'WriteTrainData' >> beam.io.WriteToTFRecord(\n",
    "              os.path.join(working_dir, TRANSFORMED_TRAIN_DATA_FILEBASE)))\n",
    "\n",
    "      # Now apply transform function to test data.  In this case we remove the\n",
    "      # trailing period at the end of each line, and also ignore the header line\n",
    "      # that is present in the test data file.\n",
    "      raw_test_data = (\n",
    "          pipeline\n",
    "          | 'ReadTestData' >> beam.io.ReadFromText(test_data_file,\n",
    "                                                   skip_header_lines=1)\n",
    "          | 'FixCommasTestData' >> beam.Map(\n",
    "              lambda line: line.replace(', ', ','))\n",
    "          | 'RemoveTrailingPeriodsTestData' >> beam.Map(lambda line: line[:-1])\n",
    "          | 'DecodeTestData' >> MapAndFilterErrors(converter.decode))\n",
    "\n",
    "      raw_test_dataset = (raw_test_data, RAW_DATA_METADATA)\n",
    "\n",
    "      transformed_test_dataset = (\n",
    "          (raw_test_dataset, transform_fn) | tft_beam.TransformDataset())\n",
    "      # Don't need transformed data schema, it's the same as before.\n",
    "      transformed_test_data, _ = transformed_test_dataset\n",
    "\n",
    "      _ = (\n",
    "          transformed_test_data\n",
    "          | 'EncodeTestData' >> beam.Map(transformed_data_coder.encode)\n",
    "          | 'WriteTestData' >> beam.io.WriteToTFRecord(\n",
    "              os.path.join(working_dir, TRANSFORMED_TEST_DATA_FILEBASE)))\n",
    "\n",
    "      # Will write a SavedModel and metadata to working_dir, which can then\n",
    "      # be read by the tft.TFTransformOutput class.\n",
    "      _ = (\n",
    "          transform_fn\n",
    "          | 'WriteTransformFn' >> tft_beam.WriteTransformFn(working_dir))\n",
    "\n",
    "# Functions for training\n",
    "\n",
    "\n",
    "def _make_training_input_fn(tf_transform_output, transformed_examples,\n",
    "                            batch_size):\n",
    "  \"\"\"Creates an input function reading from transformed data.\n",
    "\n",
    "  Args:\n",
    "    tf_transform_output: Wrapper around output of tf.Transform.\n",
    "    transformed_examples: Base filename of examples.\n",
    "    batch_size: Batch size.\n",
    "\n",
    "  Returns:\n",
    "    The input function for training or eval.\n",
    "  \"\"\"\n",
    "  def input_fn():\n",
    "    \"\"\"Input function for training and eval.\"\"\"\n",
    "    dataset = tf.data.experimental.make_batched_features_dataset(\n",
    "        file_pattern=transformed_examples,\n",
    "        batch_size=batch_size,\n",
    "        features=tf_transform_output.transformed_feature_spec(),\n",
    "        reader=tf.data.TFRecordDataset,\n",
    "        shuffle=True)\n",
    "\n",
    "    transformed_features = tf.compat.v1.data.make_one_shot_iterator(\n",
    "        dataset).get_next()\n",
    "\n",
    "    # Extract features and label from the transformed tensors.\n",
    "    # TODO(b/30367437): make transformed_labels a dict.\n",
    "    transformed_labels = transformed_features.pop(LABEL_KEY)\n",
    "\n",
    "    return transformed_features, transformed_labels\n",
    "\n",
    "  return input_fn\n",
    "\n",
    "\n",
    "def _make_serving_input_fn(tf_transform_output):\n",
    "  \"\"\"Creates an input function reading from raw data.\n",
    "\n",
    "  Args:\n",
    "    tf_transform_output: Wrapper around output of tf.Transform.\n",
    "\n",
    "  Returns:\n",
    "    The serving input function.\n",
    "  \"\"\"\n",
    "  raw_feature_spec = RAW_DATA_FEATURE_SPEC_PH.copy()\n",
    "  # Remove label since it is not available during serving.\n",
    "  raw_feature_spec.pop(LABEL_KEY)\n",
    "\n",
    "  def serving_input_fn():\n",
    "    \"\"\"Input function for serving.\"\"\"\n",
    "\n",
    "    raw_input_fn = tf.estimator.export.build_raw_serving_input_receiver_fn(\n",
    "        raw_feature_spec, default_batch_size=None)\n",
    "    serving_input_receiver = raw_input_fn()\n",
    "    raw_features = serving_input_receiver.features\n",
    "    transformed_features = tf_transform_output.transform_raw_features(\n",
    "        raw_features)\n",
    "\n",
    "    return tf.estimator.export.ServingInputReceiver(\n",
    "        transformed_features, serving_input_receiver.receiver_tensors)\n",
    "\n",
    "  return serving_input_fn\n",
    "\n",
    "\n",
    "def get_feature_columns(tf_transform_output):\n",
    "  \"\"\"Returns the FeatureColumns for the model.\n",
    "\n",
    "  Args:\n",
    "    tf_transform_output: A `TFTransformOutput` object.\n",
    "\n",
    "  Returns:\n",
    "    A list of FeatureColumns.\n",
    "  \"\"\"\n",
    "  # Wrap scalars as real valued columns.\n",
    "  real_valued_columns = [tf.feature_column.numeric_column(key, shape=())\n",
    "                         for key in NUMERIC_FEATURE_KEYS]\n",
    "\n",
    "  # Wrap categorical columns.\n",
    "  one_hot_columns = [\n",
    "      tf.feature_column.indicator_column(  # pylint: disable=g-complex-comprehension\n",
    "          tf.feature_column.categorical_column_with_vocabulary_file(\n",
    "              key=key,\n",
    "              vocabulary_file=tf_transform_output.vocabulary_file_by_name(\n",
    "                  vocab_filename=key)))\n",
    "      for key in CATEGORICAL_FEATURE_KEYS]\n",
    "\n",
    "  return real_valued_columns + one_hot_columns\n",
    "\n",
    "\n",
    "def train_and_evaluate(working_dir, num_train_instances=NUM_TRAIN_INSTANCES,\n",
    "                       num_test_instances=NUM_TEST_INSTANCES):\n",
    "  \"\"\"Train the model on training data and evaluate on test data.\n",
    "\n",
    "  Args:\n",
    "    working_dir: Directory to read transformed data and metadata from and to\n",
    "        write exported model to.\n",
    "    num_train_instances: Number of instances in train set\n",
    "    num_test_instances: Number of instances in test set\n",
    "\n",
    "  Returns:\n",
    "    The results from the estimator's 'evaluate' method\n",
    "  \"\"\"\n",
    "  tf_transform_output = tft.TFTransformOutput(working_dir)\n",
    "\n",
    "  run_config = tf.estimator.RunConfig()\n",
    "\n",
    "  estimator = tf.estimator.LinearClassifier(\n",
    "      feature_columns=get_feature_columns(tf_transform_output),\n",
    "      config=run_config,\n",
    "      loss_reduction=tf.losses.Reduction.SUM)\n",
    "\n",
    "  # Fit the model using the default optimizer.\n",
    "  train_input_fn = _make_training_input_fn(\n",
    "      tf_transform_output,\n",
    "      os.path.join(working_dir, TRANSFORMED_TRAIN_DATA_FILEBASE + '*'),\n",
    "      batch_size=TRAIN_BATCH_SIZE)\n",
    "  estimator.train(\n",
    "      input_fn=train_input_fn,\n",
    "      max_steps=TRAIN_NUM_EPOCHS * num_train_instances / TRAIN_BATCH_SIZE)\n",
    "\n",
    "  # Evaluate model on test dataset.\n",
    "  eval_input_fn = _make_training_input_fn(\n",
    "      tf_transform_output,\n",
    "      os.path.join(working_dir, TRANSFORMED_TEST_DATA_FILEBASE + '*'),\n",
    "      batch_size=1)\n",
    "\n",
    "  # Export the model.\n",
    "  serving_input_fn = _make_serving_input_fn(tf_transform_output)\n",
    "  exported_model_dir = os.path.join(working_dir, EXPORTED_MODEL_DIR)\n",
    "  estimator.export_saved_model(exported_model_dir, serving_input_fn)\n",
    "\n",
    "  return estimator.evaluate(input_fn=eval_input_fn, steps=num_test_instances)\n",
    "\n",
    "\n",
    "input_data_dir = \".\"\n",
    "working_dir = \"census_model\" # tempfile.mkdtemp(dir=input_data_dir)\n",
    "train_data_file = os.path.join(input_data_dir, 'adult.data')\n",
    "test_data_file = os.path.join(input_data_dir, 'adult.test')\n",
    "\n",
    "transform_data(train_data_file, test_data_file, working_dir)\n",
    "\n",
    "results = train_and_evaluate(working_dir)\n",
    "\n",
    "pprint.pprint(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "KlnkEoLztUT_",
    "outputId": "5cd35632-3550-4b3c-a59e-3d603387d61d"
   },
   "outputs": [],
   "source": [
    "%env MODEL_DIR=./census_model/exported_model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "mypath=\"./census_model/exported_model_dir\"\n",
    "onlyfiles = [f for f in listdir(mypath) if not isfile(join(mypath, f))]\n",
    "SAVED_MODEL_DIR=onlyfiles[0]\n",
    "SAVED_MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "k9T5e1DL2qE8",
    "outputId": "228360c4-eb86-496c-c33d-d5e1ebf2cd32"
   },
   "outputs": [],
   "source": [
    "%env PROJECT_ID=\n",
    "%env MODEL_GCS_DIR=census_model\n",
    "%env VERSION_NAME=v1\n",
    "%env MODEL_NAME=census3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "TepKiirLzgSL",
    "outputId": "c8e55527-4975-4d8e-859c-74bcb4b02d22"
   },
   "outputs": [],
   "source": [
    "%%bash -s \"$SAVED_MODEL_DIR\"\n",
    "gsutil cp -r ./census_model/exported_model_dir/$1 gs://${PROJECT_ID}/${MODEL_GCS_DIR}/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "J26rAFdGJhjJ",
    "outputId": "28729b09-befb-415f-aa23-65e3383fd4f4"
   },
   "outputs": [],
   "source": [
    "%%bash -s \"$SAVED_MODEL_DIR\"\n",
    "saved_model_cli show --dir gs://${PROJECT_ID}/${MODEL_GCS_DIR}/$1 --all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "FbOOm9NK2Wx4",
    "outputId": "5312bc4c-2a92-4795-bc36-1c6e07336723"
   },
   "outputs": [],
   "source": [
    "!gcloud ai-platform models create ${MODEL_NAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "K3CNk9Z9iwtb",
    "outputId": "a0d66db2-698c-472c-c800-ed029b97d5ec"
   },
   "outputs": [],
   "source": [
    "# run this if you need to delete and recreate the model\n",
    "#!gcloud ai-platform versions delete $VERSION_NAME --model=$MODEL_NAME --region=$REGION --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "u-tXWVD126Cq",
    "outputId": "5a6adc8a-8dbb-49e8-a762-747f8952ede0"
   },
   "outputs": [],
   "source": [
    "%%bash -s \"$SAVED_MODEL_DIR\"\n",
    "gcloud ai-platform versions create $VERSION_NAME \\\n",
    "  --model=$MODEL_NAME \\\n",
    "  --origin=$MODEL_DIR/$1 \\\n",
    "  --runtime-version=1.15 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "imFaTxCpONbz",
    "outputId": "a3346f03-f9f5-4a7a-b2e7-556482d28617"
   },
   "outputs": [],
   "source": [
    "# set this if you need to use the python client; upload a service account json file\n",
    "#!export GOOGLE_APPLICATION_CREDENTIALS="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "colab_type": "code",
    "id": "IIr24W-vNsVf",
    "outputId": "d7762e20-bf4e-4d67-ecd6-dc53d8d7e0d1"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import base64\n",
    "import json\n",
    "\n",
    "from google.api_core.client_options import ClientOptions\n",
    "from googleapiclient import discovery\n",
    "import six\n",
    "\n",
    "def predict_json(project, model, instances, version='v1'):\n",
    "    \"\"\"Send json data to a deployed model for prediction.\n",
    "\n",
    "    Args:\n",
    "        project (str): project where the AI Platform Model is deployed.\n",
    "        model (str): model name.\n",
    "        instances ([Mapping[str: Any]]): Keys should be the names of Tensors\n",
    "            your deployed model expects as inputs. Values should be datatypes\n",
    "            convertible to Tensors, or (potentially nested) lists of datatypes\n",
    "            convertible to tensors.\n",
    "        version: str, version of the model to target.\n",
    "    Returns:\n",
    "        Mapping[str: any]: dictionary of prediction results defined by the\n",
    "            model.\n",
    "    \"\"\"\n",
    "    # Create the AI Platform service object.\n",
    "    # To authenticate set the environment variable\n",
    "    # GOOGLE_APPLICATION_CREDENTIALS=<path_to_service_account_file>\n",
    "\n",
    "\n",
    "    ml = discovery.build('ml', 'v1')\n",
    "    name = 'projects/{}/models/{}'.format(project, model)\n",
    "\n",
    "    if version is not None:\n",
    "        name += '/versions/{}'.format(version)\n",
    "    request_body = { 'instances': [instances] }\n",
    "    request = ml.projects().predict(\n",
    "        name=name,\n",
    "        body=request_body)\n",
    "\n",
    "    response = request.execute()\n",
    "\n",
    "    if 'error' in response:\n",
    "        raise RuntimeError(response['error'])\n",
    "\n",
    "    return response['predictions']\n",
    "\n",
    "predict_json(os.getenv('PROJECT_ID'), os.getenv('MODEL_NAME'),  \n",
    "             {\"age\":50.0, \"workclass\":\"Self-emp-not-inc\", \"education\":\"Bachelors\", \"education_num\":13.0, \n",
    "              \"marital_status\":\"Married-civ-spouse\", \"occupation\":\"Exec-managerial\", \"relationship\":\"Husband\", \n",
    "              \"race\":\"White\", \"sex\":\"Male\", \"capital_gain\":0.0, \"capital_loss\":0.0, \"hours_per_week\":13.0, \"native_country\":\"United-States\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Ehho7n87Gq0p",
    "outputId": "795d8c88-a28b-42dd-ea5d-153749aae4dd"
   },
   "outputs": [],
   "source": [
    "%%writefile test.json\n",
    "{\"age\":50.0, \"workclass\":\"Self-emp-not-inc\", \"education\":\"Bachelors\", \"education_num\":13.0, \"marital_status\":\"Married-civ-spouse\", \"occupation\":\"Exec-managerial\", \"relationship\":\"Husband\", \"race\":\"White\", \"sex\":\"Male\", \"capital_gain\":0.0, \"capital_loss\":0.0, \"hours_per_week\":13.0, \"native_country\":\"United-States\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "colab_type": "code",
    "id": "YBsDCz_0BI7a",
    "outputId": "ee0a6b0f-0baf-41b4-aa62-acf2ea574f44"
   },
   "outputs": [],
   "source": [
    "!gcloud ai-platform predict --model census3 --version v1 --json-instances test.json\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TF Census - JSON Serving Input Fn.ipynb",
   "provenance": []
  },
  "environment": {
   "name": "tf-gpu.1-15.m55",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf-gpu.1-15:m55"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
