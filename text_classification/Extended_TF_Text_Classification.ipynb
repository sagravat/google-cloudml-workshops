{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Extended_TF_Text_Classification.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/sagravat/google-cloudml-workshops/blob/master/text_classification/Extended_TF_Text_Classification.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "gQM5BOxlPx97",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Text Classification using TensorFlow and Google Cloud\n",
        "\n",
        "This [bigquery-public-data:hacker_news](https://cloud.google.com/bigquery/public-data/hacker-news) contains all stories and comments from Hacker News from its launch in 2006.  Each story contains a story id, url, the title of the story, tthe author that made the post, when it was written, and the number of points the story received.\n",
        "\n",
        "The objective is, given the title of the story, we want to build an ML model that can predict the source of this story.\n",
        "\n",
        "### This notebook illustrates:\n",
        "* Creating a ML datasets using Dataflow\n",
        "* Create classification models with TensforFlow Estimaor APIs & TF.hub\n",
        "* Train the best model using Cloud ML Engine\n",
        "* Deploy the model on Cloud ML Engine and perform predictions\n"
      ]
    },
    {
      "metadata": {
        "id": "0YxA5Y6NPx99",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Note: Use Python 3 Kernel "
      ]
    },
    {
      "metadata": {
        "id": "RC3bVB3CPx9-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "493e4577-c65b-4831-c8f0-9ab3f12e39ec"
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "echo \"PROJECT_ID: $(gcloud config get-value project)\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PROJECT_ID: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "(unset)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "Hlkb6bdzPx-A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# change these to the project id\n",
        "BUCKET = 'agravat-demo'\n",
        "PROJECT = 'agravat-demo'\n",
        "REGION = 'us-central1'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HZHTr3YzPx-C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['BUCKET'] = BUCKET\n",
        "os.environ['PROJECT'] = PROJECT\n",
        "os.environ['REGION'] = REGION"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4rAZArXPPx-F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "if ! gsutil ls | grep -q gs://${BUCKET}/; then\n",
        "  gsutil mb -l ${REGION} gs://${BUCKET}\n",
        "fi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T7gyZXn8Px-H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "class Params:\n",
        "    pass\n",
        "\n",
        "\n",
        "Params.PLATFORM = 'GCP' # local | GCP\n",
        "\n",
        "Params.DATA_DIR = 'data/news'  if Params.PLATFORM == 'local' else 'gs://{}/data/news'.format(BUCKET)\n",
        "Params.TRANSFORMED_DATA_DIR = os.path.join(Params.DATA_DIR, 'transformed')\n",
        "\n",
        "Params.RAW_TRAIN_DATA_FILE_PREFEX = os.path.join(Params.DATA_DIR, 'train')\n",
        "Params.RAW_EVAL_DATA_FILE_PREFEX = os.path.join(Params.DATA_DIR, 'eval')\n",
        "\n",
        "Params.MODELS_DIR = 'models/news' if Params.PLATFORM == 'local' else 'gs://{}/models/news'.format(BUCKET)\n",
        "\n",
        "Params.TEMP_DIR = os.path.join(Params.DATA_DIR, 'tmp')\n",
        "\n",
        "Params.TRANSFORM = True\n",
        "\n",
        "Params.TRAIN = True\n",
        "\n",
        "Params.RESUME_TRAINING = False\n",
        "\n",
        "Params.EAGER = False\n",
        "\n",
        "if Params.EAGER:\n",
        "    tf.enable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ECm4neO4Px-J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Create a ML Data Files using Dataflow\n",
        "\n",
        "The data processing pipeline will do the following:\n",
        "1. Read the data (key, title, source) from BigQuery\n",
        "2. Process text (if needed) and convert each BQ raw to tsv\n",
        "3. Save data to tsv files"
      ]
    },
    {
      "metadata": {
        "id": "a0N28Z4fPx-J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1. Source Query"
      ]
    },
    {
      "metadata": {
        "id": "tpj-TMcdPx-K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "bq_query = '''\n",
        "SELECT\n",
        "    key,\n",
        "    REGEXP_REPLACE(title, '[^a-zA-Z0-9 $.-]', ' ') AS title, \n",
        "    source\n",
        "FROM\n",
        "(\n",
        "    SELECT\n",
        "        ARRAY_REVERSE(SPLIT(REGEXP_EXTRACT(url, '.*://(.[^/]+)/'), '.'))[OFFSET(1)] AS source,\n",
        "        title,\n",
        "        ABS(FARM_FINGERPRINT(title)) AS Key\n",
        "    FROM\n",
        "      `bigquery-public-data.hacker_news.stories`\n",
        "    WHERE\n",
        "      REGEXP_CONTAINS(REGEXP_EXTRACT(url, '.*://(.[^/]+)/'), '.com$')\n",
        "      AND LENGTH(title) > 10\n",
        ")\n",
        "WHERE (source = 'github' OR source = 'nytimes' OR source = 'techcrunch' OR source = 'medium' OR source = 'wsj' OR source = 'wired' OR source = 'blogspot' or source = 'arstechnica')\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G5mbaOoZPx-N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2. Beam Pipeline"
      ]
    },
    {
      "metadata": {
        "id": "03w6xsYcPx-O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import apache_beam as beam\n",
        "\n",
        "\n",
        "def to_tsv(bq_row):\n",
        "    \n",
        "    CSV_HEADER = 'key,title,source'.split(',')\n",
        "    \n",
        "    ### process bq_row['title'] \n",
        "    \n",
        "    csv_row = '\\t'.join([str(bq_row[column]) for column in CSV_HEADER])\n",
        "    return csv_row\n",
        "\n",
        "\n",
        "\n",
        "def run_pipeline(runner, opts):\n",
        "  \n",
        "    pipeline = beam.Pipeline(runner, options=opts)\n",
        "    \n",
        "    print(\"Sink train data files: {}\".format(Params.RAW_TRAIN_DATA_FILE_PREFEX))\n",
        "    print(\"Sink data files: {}\".format(Params.RAW_EVAL_DATA_FILE_PREFEX))\n",
        "    print(\"Temporary directory: {}\".format(Params.TEMP_DIR))\n",
        "    print(\"\")\n",
        "    \n",
        "    for step in ['train', 'eval']:\n",
        "        \n",
        "        if step == 'train':\n",
        "            source_query = 'SELECT * FROM ({}) WHERE MOD(key,100) <= 75'.format(bq_query)\n",
        "            sink_location = Params.RAW_TRAIN_DATA_FILE_PREFEX\n",
        "        else:\n",
        "            source_query = 'SELECT * FROM ({}) WHERE MOD(key,100) > 75'.format(bq_query)\n",
        "            sink_location = Params.RAW_EVAL_DATA_FILE_PREFEX\n",
        "            \n",
        "        (\n",
        "            pipeline \n",
        "           | '{} - Read from BigQuery'.format(step) >> beam.io.Read(beam.io.BigQuerySource(query=source_query, use_standard_sql=True))\n",
        "           | '{} - Process to TSV'.format(step) >> beam.Map(to_tsv)\n",
        "           | '{} - Write to TSV '.format(step) >> beam.io.Write(beam.io.WriteToText(sink_location,\n",
        "                                                                file_name_suffix='.tsv', num_shards=5))\n",
        "        )\n",
        "        \n",
        "    job = pipeline.run()\n",
        "    if runner == 'DirectRunner':\n",
        "        job.wait_until_finish()\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S71299lQVJ2i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/Users/agravat/Downloads/agravat-demo-ca55740f142c.json'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xM7zarz7Px-R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 5. Run Pipeline"
      ]
    },
    {
      "metadata": {
        "id": "36BRwQ70Px-R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "3abebe5d-af8b-4c98-d6ae-bff0906c1723"
      },
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "import shutil\n",
        "\n",
        "job_name = 'preprocess-hackernews-data' + '-' + datetime.utcnow().strftime('%y%m%d-%H%M%S')\n",
        "\n",
        "options = {\n",
        "    'region': REGION,\n",
        "    'staging_location': os.path.join(Params.TEMP_DIR, 'staging'),\n",
        "    'temp_location': Params.TEMP_DIR,\n",
        "    'job_name': job_name,\n",
        "    'project': PROJECT\n",
        "}\n",
        "\n",
        "opts = beam.pipeline.PipelineOptions(flags=[], **options)\n",
        "runner = 'DirectRunner' if Params.PLATFORM == 'local' else 'DataflowRunner'\n",
        "\n",
        "if Params.TRANSFORM:\n",
        "    \n",
        "    if Params.PLATFORM == 'local':\n",
        "        shutil.rmtree(Params.DATA_DIR, ignore_errors=True)\n",
        "    \n",
        "    print 'Launching {} job {} ... hang on'.format(runner, job_name)\n",
        "\n",
        "    run_pipeline(runner, opts)\n",
        "    print \"Pipline completed.\"\n",
        "else:\n",
        "    print \"Transformation skipped!\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Launching DataflowRunner job preprocess-hackernews-data-180812-225046 ... hang on\n",
            "Sink train data files: gs://agravat-demo/data/news/train\n",
            "Sink data files: gs://agravat-demo/data/news/eval\n",
            "Temporary directory: gs://agravat-demo/data/news/tmp\n",
            "\n",
            "Pipline completed.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hPmT7DwoPx-T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "c8f23413-7e5d-4352-a8e9-ba57e4259495"
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "gsutil ls gs://agravat-demo/data/news/train*\n",
        "# echo \"\"\n",
        "# head data/news/train-00000-of-00005.tsv"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gs://agravat-demo/data/news/train-00000-of-00005.tsv\n",
            "gs://agravat-demo/data/news/train-00001-of-00005.tsv\n",
            "gs://agravat-demo/data/news/train-00002-of-00005.tsv\n",
            "gs://agravat-demo/data/news/train-00003-of-00005.tsv\n",
            "gs://agravat-demo/data/news/train-00004-of-00005.tsv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zxF8A852Px-V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## TF Text Classification Model with TF Hub for Text Encoding"
      ]
    },
    {
      "metadata": {
        "id": "FaD99H0wPx-W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1. Define metadata & input function"
      ]
    },
    {
      "metadata": {
        "id": "dUjqFJ3kPx-W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4635712f-75b7-4307-a358-a245b4e69ffc"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import data\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.10.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2uSiwJC-Px-Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "RAW_HEADER = 'key,title,source'.split(',')\n",
        "RAW_DEFAULTS = [['NA'],['NA'],['NA']]\n",
        "TARGET_FEATRUE_NAME = 'source'\n",
        "TARGET_LABELS = ['github', 'nytimes', 'techcrunch']\n",
        "TEXT_FEATURE_NAME = 'title'\n",
        "KEY_COLUMN = 'key'\n",
        "\n",
        "def parse_tsv(tsv_row):\n",
        "    \n",
        "    columns = tf.decode_csv(tsv_row, record_defaults=RAW_DEFAULTS, field_delim='\\t')\n",
        "    features = dict(zip(RAW_HEADER, columns))\n",
        "    \n",
        "    features.pop(KEY_COLUMN)\n",
        "    target = features.pop(TARGET_FEATRUE_NAME)\n",
        "    \n",
        "    return features, target\n",
        "\n",
        "\n",
        "def generate_tsv_input_fn(files_pattern, \n",
        "                          mode=tf.estimator.ModeKeys.EVAL, \n",
        "                          num_epochs=1, \n",
        "                          batch_size=200):\n",
        "    \n",
        "\n",
        "    def _input_fn():\n",
        "        \n",
        "        #file_names = data.Dataset.list_files(files_pattern)\n",
        "        file_names = tf.matching_files(files_pattern)\n",
        "\n",
        "        if Params.EAGER:\n",
        "            print(file_names)\n",
        "\n",
        "        dataset = data.TextLineDataset(file_names)\n",
        "\n",
        "        dataset = dataset.apply(\n",
        "                tf.contrib.data.shuffle_and_repeat(count=num_epochs,\n",
        "                                                   buffer_size=batch_size*2)\n",
        "        )\n",
        "\n",
        "        dataset = dataset.apply(\n",
        "                tf.contrib.data.map_and_batch(parse_tsv, \n",
        "                                              batch_size=batch_size, \n",
        "                                              num_parallel_batches=2)\n",
        "        )\n",
        "\n",
        "        datset = dataset.prefetch(batch_size)\n",
        "\n",
        "        if Params.EAGER:\n",
        "            return dataset\n",
        "\n",
        "        iterator = dataset.make_one_shot_iterator()\n",
        "        features, target = iterator.get_next()\n",
        "        return features, target\n",
        "    \n",
        "    return _input_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i1eJ_AYYPx-Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-hub"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oWDvdb0iPx-b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2. Create feature columns"
      ]
    },
    {
      "metadata": {
        "id": "IHuSCul-Px-b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d63e13b3-0e6a-4f03-9854-cd2092ca3048"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow_hub as hub\n",
        "print(hub.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hMhXHs80Px-c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_feature_columns(hparams):\n",
        "    \n",
        "    title_embeding_column = hub.text_embedding_column(\n",
        "        \"title\", \"https://tfhub.dev/google/universal-sentence-encoder/1\")\n",
        "    \n",
        "    feature_columns = [title_embeding_column]\n",
        "    \n",
        "    print(\"feature columns: \\n {}\".format(feature_columns))\n",
        "    print(\"\")\n",
        "    \n",
        "    return feature_columns\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wYwPAtojPx-e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3. Create a model using a the  premade DNNClassifer"
      ]
    },
    {
      "metadata": {
        "id": "GgOg3vi-Px-f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_estimator_hub(hparams, run_config):\n",
        "    \n",
        "    feature_columns = create_feature_columns(hparams)\n",
        "    \n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=hparams.learning_rate)\n",
        "    \n",
        "    estimator = tf.estimator.DNNClassifier(\n",
        "        feature_columns=feature_columns,\n",
        "        n_classes =len(TARGET_LABELS),\n",
        "        label_vocabulary=TARGET_LABELS,\n",
        "        hidden_units=hparams.hidden_units,\n",
        "        optimizer=optimizer,\n",
        "        config=run_config\n",
        "    )\n",
        "    \n",
        "    \n",
        "    return estimator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hrsVKNyaPx-g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 4. Define experiment"
      ]
    },
    {
      "metadata": {
        "id": "0pQOTLSyPx-h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### a) HParams and RunConfig"
      ]
    },
    {
      "metadata": {
        "id": "Ou0Wo8xnPx-i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "TRAIN_SIZE = 73124\n",
        "NUM_EPOCHS = 10\n",
        "BATCH_SIZE = 1000\n",
        "\n",
        "TOTAL_STEPS = (TRAIN_SIZE/BATCH_SIZE)*NUM_EPOCHS\n",
        "EVAL_EVERY_SEC = 60\n",
        "\n",
        "hparams  = tf.contrib.training.HParams(\n",
        "    num_epochs=NUM_EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    trainable_embedding=False,\n",
        "    learning_rate=0.01,\n",
        "    hidden_units=[256, 128],\n",
        "    max_steps=TOTAL_STEPS\n",
        ")\n",
        "\n",
        "MODEL_NAME = 'dnn_estimator_hub' \n",
        "model_dir = os.path.join(Params.MODELS_DIR, MODEL_NAME)\n",
        "\n",
        "run_config = tf.estimator.RunConfig(\n",
        "    tf_random_seed=19830610,\n",
        "    log_step_count_steps=1000,\n",
        "    save_checkpoints_secs=EVAL_EVERY_SEC,\n",
        "    keep_checkpoint_max=1,\n",
        "    model_dir=model_dir\n",
        ")\n",
        "\n",
        "\n",
        "print(hparams)\n",
        "print(\"\")\n",
        "print(\"Model Directory:\", run_config.model_dir)\n",
        "print(\"Dataset Size:\", TRAIN_SIZE)\n",
        "print(\"Batch Size:\", BATCH_SIZE)\n",
        "print(\"Steps per Epoch:\",TRAIN_SIZE/BATCH_SIZE)\n",
        "print(\"Total Steps:\", TOTAL_STEPS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4OHlZcdSPx-l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### b) Serving function"
      ]
    },
    {
      "metadata": {
        "id": "B-pSulRmPx-l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_serving_input_fn():\n",
        "    \n",
        "    def _serving_fn():\n",
        "    \n",
        "        receiver_tensor = {\n",
        "          'title': tf.placeholder(dtype=tf.string, shape=[None])\n",
        "        }\n",
        "\n",
        "        return tf.estimator.export.ServingInputReceiver(\n",
        "            receiver_tensor, receiver_tensor)\n",
        "    \n",
        "    return _serving_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8hbMi-RvPx-p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### c) TrainSpec & EvalSpec"
      ]
    },
    {
      "metadata": {
        "id": "4O0WMNH0Px-p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_spec = tf.estimator.TrainSpec(\n",
        "    input_fn = generate_tsv_input_fn(\n",
        "        Params.RAW_TRAIN_DATA_FILE_PREFEX+\"*\",\n",
        "        mode = tf.estimator.ModeKeys.TRAIN,\n",
        "        num_epochs=hparams.num_epochs,\n",
        "        batch_size=hparams.batch_size\n",
        "    ),\n",
        "    max_steps=hparams.max_steps,\n",
        "    hooks=None\n",
        ")\n",
        "\n",
        "eval_spec = tf.estimator.EvalSpec(\n",
        "    input_fn = generate_tsv_input_fn(\n",
        "        Params.RAW_EVAL_DATA_FILE_PREFEX+\"*\",\n",
        "        mode=tf.estimator.ModeKeys.EVAL,\n",
        "        num_epochs=1,\n",
        "        batch_size=hparams.batch_size\n",
        "    ),\n",
        "    exporters=[tf.estimator.LatestExporter(\n",
        "        name=\"estimate\", # the name of the folder in which the model will be exported to under export\n",
        "        serving_input_receiver_fn=generate_serving_input_fn(),\n",
        "        exports_to_keep=1,\n",
        "        as_text=False)],\n",
        "    steps=None,\n",
        "    throttle_secs=EVAL_EVERY_SEC\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IvsBCO2ePx-q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 5. Run experiment"
      ]
    },
    {
      "metadata": {
        "id": "xCWnBQqUPx-r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "import shutil\n",
        "\n",
        "if Params.TRAIN:\n",
        "    if not Params.RESUME_TRAINING:\n",
        "        print(\"Removing previous training artefacts...\")\n",
        "        shutil.rmtree(model_dir, ignore_errors=True)\n",
        "    else:\n",
        "        print(\"Resuming training...\") \n",
        "\n",
        "\n",
        "    tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "    time_start = datetime.utcnow() \n",
        "    print(\"Experiment started at {}\".format(time_start.strftime(\"%H:%M:%S\")))\n",
        "    print(\".......................................\") \n",
        "\n",
        "    estimator = create_estimator_hub(hparams, run_config)\n",
        "\n",
        "    tf.estimator.train_and_evaluate(\n",
        "        estimator=estimator,\n",
        "        train_spec=train_spec, \n",
        "        eval_spec=eval_spec\n",
        "    )\n",
        "\n",
        "    time_end = datetime.utcnow() \n",
        "    print(\".......................................\")\n",
        "    print(\"Experiment finished at {}\".format(time_end.strftime(\"%H:%M:%S\")))\n",
        "    print(\"\")\n",
        "    time_elapsed = time_end - time_start\n",
        "    print(\"Experiment elapsed time: {} seconds\".format(time_elapsed.total_seconds()))\n",
        "else:\n",
        "    print(\"Training was skipped!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wH6HQXhTPx-v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 6. Evaluate the model"
      ]
    },
    {
      "metadata": {
        "id": "af1hqV86Px-v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "TRAIN_SIZE = 73124\n",
        "VALID_SIZE = 23079\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "estimator = create_estimator_hub(hparams, run_config)\n",
        "\n",
        "train_metrics = estimator.evaluate(\n",
        "    input_fn = generate_tsv_input_fn(\n",
        "        files_pattern= Params.RAW_TRAIN_DATA_FILE_PREFEX+\"*\", \n",
        "        mode= tf.estimator.ModeKeys.EVAL,\n",
        "        batch_size= TRAIN_SIZE), \n",
        "    steps=1\n",
        ")\n",
        "\n",
        "\n",
        "print(\"############################################################################################\")\n",
        "print(\"# Train Measures: {}\".format(train_metrics))\n",
        "print(\"############################################################################################\")\n",
        "\n",
        "eval_metrics = estimator.evaluate(\n",
        "    input_fn=generate_tsv_input_fn(\n",
        "        files_pattern=Params.RAW_EVAL_DATA_FILE_PREFEX+\"*\", \n",
        "        mode= tf.estimator.ModeKeys.EVAL,\n",
        "        batch_size= TRAIN_SIZE), \n",
        "    steps=1\n",
        ")\n",
        "print(\"\")\n",
        "print(\"############################################################################################\")\n",
        "print(\"# Valid Measures: {}\".format(eval_metrics))\n",
        "print(\"############################################################################################\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AByemqmpPx-x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 7. Use SavedModel for predictions "
      ]
    },
    {
      "metadata": {
        "id": "-yTw1hGHPx-x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "export_dir = model_dir +\"/export/estimate/\"\n",
        "saved_model_dir = os.path.join(export_dir, os.listdir(export_dir)[0])\n",
        "\n",
        "print(saved_model_dir)\n",
        "print(\"\")\n",
        "\n",
        "predictor_fn = tf.contrib.predictor.from_saved_model(\n",
        "    export_dir = saved_model_dir,\n",
        "    signature_def_key=\"predict\"\n",
        ")\n",
        "\n",
        "output = predictor_fn(\n",
        "    {\n",
        "        'title':[\n",
        "            'Microsoft and Google are joining forces for a new AI framework',\n",
        "            'A new version of Python is mind blowing',\n",
        "            'EU is investigating new data privacy policies'\n",
        "        ]\n",
        "        \n",
        "    }\n",
        ")\n",
        "print(output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m2hGYITHPx-y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}