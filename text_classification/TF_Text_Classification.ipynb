{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification using TensorFlow and Google Cloud\n",
    "\n",
    "This [bigquery-public-data:hacker_news](https://cloud.google.com/bigquery/public-data/hacker-news) contains all stories and comments from Hacker News from its launch in 2006.  Each story contains a story id, url, the title of the story, tthe author that made the post, when it was written, and the number of points the story received.\n",
    "\n",
    "The objective is, given the title of the story, we want to build an ML model that can predict the source of this story.\n",
    "\n",
    "### This notebook illustrates:\n",
    "* Creating a ML datasets using Dataflow\n",
    "* Create classification models with TensforFlow Estimaor APIs & TF.hub\n",
    "* Train the best model using Cloud ML Engine\n",
    "* Deploy the model on Cloud ML Engine and perform predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "echo \"PROJECT_ID: $(gcloud config get-value project)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change these to the project id\n",
    "BUCKET = ''\n",
    "PROJECT = ''\n",
    "REGION = 'us-central1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['REGION'] = REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if ! gsutil ls | grep -q gs://${BUCKET}/; then\n",
    "  gsutil mb -l ${REGION} gs://${BUCKET}\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class Params:\n",
    "    pass\n",
    "\n",
    "\n",
    "Params.PLATFORM = 'local' # local | GCP\n",
    "\n",
    "Params.DATA_DIR = 'data/news'  if Params.PLATFORM == 'local' else 'gs://{}/data/news'.format(Params.BUCKET)\n",
    "Params.TRANSFORMED_DATA_DIR = os.path.join(Params.DATA_DIR, 'transformed')\n",
    "\n",
    "Params.RAW_TRAIN_DATA_FILE_PREFEX = os.path.join(Params.DATA_DIR, 'train')\n",
    "Params.RAW_EVAL_DATA_FILE_PREFEX = os.path.join(Params.DATA_DIR, 'eval')\n",
    "\n",
    "Params.MODELS_DIR = 'models/news' if Params.PLATFORM == 'local' else 'gs://{}/models/news'.format(Params.BUCKET)\n",
    "\n",
    "Params.TEMP_DIR = os.path.join(Params.DATA_DIR, 'tmp')\n",
    "\n",
    "Params.TRANSFORM = True\n",
    "\n",
    "Params.TRAIN = True\n",
    "\n",
    "Params.RESUME_TRAINING = False\n",
    "\n",
    "Params.EAGER = False\n",
    "\n",
    "if Params.EAGER:\n",
    "    tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a ML Data Files using Dataflow\n",
    "\n",
    "The data processing pipeline will do the following:\n",
    "1. Read the data (key, title, source) from BigQuery\n",
    "2. Process text (if needed) and convert each BQ raw to tsv\n",
    "3. Save data to tsv files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Source Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bq_query = '''\n",
    "SELECT\n",
    "    key,\n",
    "    REGEXP_REPLACE(title, '[^a-zA-Z0-9 $.-]', ' ') AS title, \n",
    "    source\n",
    "FROM\n",
    "(\n",
    "    SELECT\n",
    "        ARRAY_REVERSE(SPLIT(REGEXP_EXTRACT(url, '.*://(.[^/]+)/'), '.'))[OFFSET(1)] AS source,\n",
    "        title,\n",
    "        ABS(FARM_FINGERPRINT(title)) AS Key\n",
    "    FROM\n",
    "      `bigquery-public-data.hacker_news.stories`\n",
    "    WHERE\n",
    "      REGEXP_CONTAINS(REGEXP_EXTRACT(url, '.*://(.[^/]+)/'), '.com$')\n",
    "      AND LENGTH(title) > 10\n",
    ")\n",
    "WHERE (source = 'github' OR source = 'nytimes' OR source = 'techcrunch')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Beam Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "\n",
    "def to_tsv(bq_row):\n",
    "    \n",
    "    CSV_HEADER = 'key,title,source'.split(',')\n",
    "    \n",
    "    ### process bq_row['title'] \n",
    "    \n",
    "    csv_row = '\\t'.join([str(bq_row[column]) for column in CSV_HEADER])\n",
    "    return csv_row\n",
    "\n",
    "\n",
    "\n",
    "def run_pipeline(runner, opts):\n",
    "  \n",
    "    pipeline = beam.Pipeline(runner, options=opts)\n",
    "    \n",
    "    print(\"Sink train data files: {}\".format(Params.RAW_TRAIN_DATA_FILE_PREFEX))\n",
    "    print(\"Sink data files: {}\".format(Params.RAW_EVAL_DATA_FILE_PREFEX))\n",
    "    print(\"Temporary directory: {}\".format(Params.TEMP_DIR))\n",
    "    print(\"\")\n",
    "    \n",
    "    for step in ['train', 'eval']:\n",
    "        \n",
    "        if step == 'train':\n",
    "            source_query = 'SELECT * FROM ({}) WHERE MOD(key,100) <= 75'.format(bq_query)\n",
    "            sink_location = Params.RAW_TRAIN_DATA_FILE_PREFEX\n",
    "        else:\n",
    "            source_query = 'SELECT * FROM ({}) WHERE MOD(key,100) > 75'.format(bq_query)\n",
    "            sink_location = Params.RAW_EVAL_DATA_FILE_PREFEX\n",
    "            \n",
    "        (\n",
    "            pipeline \n",
    "           | '{} - Read from BigQuery'.format(step) >> beam.io.Read(beam.io.BigQuerySource(query=source_query, use_standard_sql=True))\n",
    "           | '{} - Process to TSV'.format(step) >> beam.Map(to_tsv)\n",
    "           | '{} - Write to TSV '.format(step) >> beam.io.Write(beam.io.WriteToText(sink_location,\n",
    "                                                                file_name_suffix='.tsv', num_shards=5))\n",
    "        )\n",
    "        \n",
    "    job = pipeline.run()\n",
    "    if runner == 'DirectRunner':\n",
    "        job.wait_until_finish()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Run Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import shutil\n",
    "\n",
    "job_name = 'preprocess-hackernews-data' + '-' + datetime.utcnow().strftime('%y%m%d-%H%M%S')\n",
    "\n",
    "options = {\n",
    "    'region': REGION,\n",
    "    'staging_location': os.path.join(Params.TEMP_DIR, 'staging'),\n",
    "    'temp_location': Params.TEMP_DIR,\n",
    "    'job_name': job_name,\n",
    "    'project': PROJECT\n",
    "}\n",
    "\n",
    "opts = beam.pipeline.PipelineOptions(flags=[], **options)\n",
    "runner = 'DirectRunner' if Params.PLATFORM == 'local' else 'DirectRunner'\n",
    "\n",
    "if Params.TRANSFORM:\n",
    "    \n",
    "    if Params.PLATFORM == 'local':\n",
    "        shutil.rmtree(Params.DATA_DIR, ignore_errors=True)\n",
    "    \n",
    "    print 'Launching {} job {} ... hang on'.format(runner, job_name)\n",
    "\n",
    "    run_pipeline(runner, opts)\n",
    "    print \"Pipline completed.\"\n",
    "else:\n",
    "    print \"Transformation skipped!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "ls data/news\n",
    "echo \"\"\n",
    "head data/news/train-00000-of-00005.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF Text Classification Model with TF Hub for Text Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Define metadata & input function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import data\n",
    "print tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_HEADER = 'key,title,source'.split(',')\n",
    "RAW_DEFAULTS = [['NA'],['NA'],['NA']]\n",
    "TARGET_FEATRUE_NAME = 'source'\n",
    "TARGET_LABELS = ['github', 'nytimes', 'techcrunch']\n",
    "TEXT_FEATURE_NAME = 'title'\n",
    "KEY_COLUMN = 'key'\n",
    "\n",
    "def parse_tsv(tsv_row):\n",
    "    \n",
    "    columns = tf.decode_csv(tsv_row, record_defaults=RAW_DEFAULTS, field_delim='\\t')\n",
    "    features = dict(zip(RAW_HEADER, columns))\n",
    "    \n",
    "    features.pop(KEY_COLUMN)\n",
    "    target = features.pop(TARGET_FEATRUE_NAME)\n",
    "    \n",
    "    return features, target\n",
    "\n",
    "\n",
    "def generate_tsv_input_fn(files_pattern, \n",
    "                          mode=tf.estimator.ModeKeys.EVAL, \n",
    "                          num_epochs=1, \n",
    "                          batch_size=200):\n",
    "    \n",
    "\n",
    "    def _input_fn():\n",
    "        \n",
    "        #file_names = data.Dataset.list_files(files_pattern)\n",
    "        file_names = tf.matching_files(files_pattern)\n",
    "\n",
    "        if Params.EAGER:\n",
    "            print file_names\n",
    "\n",
    "        dataset = data.TextLineDataset(file_names)\n",
    "\n",
    "        dataset = dataset.apply(\n",
    "                tf.contrib.data.shuffle_and_repeat(count=num_epochs,\n",
    "                                                   buffer_size=batch_size*2)\n",
    "        )\n",
    "\n",
    "        dataset = dataset.apply(\n",
    "                tf.contrib.data.map_and_batch(parse_tsv, \n",
    "                                              batch_size=batch_size, \n",
    "                                              num_parallel_batches=2)\n",
    "        )\n",
    "\n",
    "        datset = dataset.prefetch(batch_size)\n",
    "\n",
    "        if Params.EAGER:\n",
    "            return dataset\n",
    "\n",
    "        iterator = dataset.make_one_shot_iterator()\n",
    "        features, target = iterator.get_next()\n",
    "        return features, target\n",
    "    \n",
    "    return _input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create feature columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "print hub.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_columns(hparams):\n",
    "    \n",
    "    title_embeding_column = hub.text_embedding_column(\n",
    "        \"title\", \"https://tfhub.dev/google/universal-sentence-encoder/1\",\n",
    "        trainable=hparams.trainable_embedding)\n",
    "    \n",
    "    feature_columns = [title_embeding_column]\n",
    "    \n",
    "    print \"feature columns: \\n {}\".format(feature_columns)\n",
    "    print \"\"\n",
    "    \n",
    "    return feature_columns\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create a model using a the  premade DNNClassifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_estimator_hub(hparams, run_config):\n",
    "    \n",
    "    feature_columns = create_feature_columns(hparams)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=hparams.learning_rate)\n",
    "    \n",
    "    estimator = tf.estimator.DNNClassifier(\n",
    "        feature_columns=feature_columns,\n",
    "        n_classes =len(TARGET_LABELS),\n",
    "        label_vocabulary=TARGET_LABELS,\n",
    "        hidden_units=hparams.hidden_units,\n",
    "        optimizer=optimizer,\n",
    "        config=run_config\n",
    "    )\n",
    "    \n",
    "    \n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Define experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a) HParams and RunConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 73124\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 1000\n",
    "\n",
    "TOTAL_STEPS = (TRAIN_SIZE/BATCH_SIZE)*NUM_EPOCHS\n",
    "EVAL_EVERY_SEC = 60\n",
    "\n",
    "hparams  = tf.contrib.training.HParams(\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    trainable_embedding=False,\n",
    "    learning_rate=0.01,\n",
    "    hidden_units=[256, 128],\n",
    "    max_steps=TOTAL_STEPS\n",
    ")\n",
    "\n",
    "MODEL_NAME = 'dnn_estimator_hub' \n",
    "model_dir = os.path.join(Params.MODELS_DIR, MODEL_NAME)\n",
    "\n",
    "run_config = tf.estimator.RunConfig(\n",
    "    tf_random_seed=19830610,\n",
    "    log_step_count_steps=1000,\n",
    "    save_checkpoints_secs=EVAL_EVERY_SEC,\n",
    "    keep_checkpoint_max=1,\n",
    "    model_dir=model_dir\n",
    ")\n",
    "\n",
    "\n",
    "print(hparams)\n",
    "print(\"\")\n",
    "print(\"Model Directory:\", run_config.model_dir)\n",
    "print(\"Dataset Size:\", TRAIN_SIZE)\n",
    "print(\"Batch Size:\", BATCH_SIZE)\n",
    "print(\"Steps per Epoch:\",TRAIN_SIZE/BATCH_SIZE)\n",
    "print(\"Total Steps:\", TOTAL_STEPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b) Serving function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_serving_input_fn():\n",
    "    \n",
    "    def _serving_fn():\n",
    "    \n",
    "        receiver_tensor = {\n",
    "          'title': tf.placeholder(dtype=tf.string, shape=[None])\n",
    "        }\n",
    "\n",
    "        return tf.estimator.export.ServingInputReceiver(\n",
    "            receiver_tensor, receiver_tensor)\n",
    "    \n",
    "    return _serving_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### c) TrainSpec & EvalSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_spec = tf.estimator.TrainSpec(\n",
    "    input_fn = generate_tsv_input_fn(\n",
    "        Params.RAW_TRAIN_DATA_FILE_PREFEX+\"*\",\n",
    "        mode = tf.estimator.ModeKeys.TRAIN,\n",
    "        num_epochs=hparams.num_epochs,\n",
    "        batch_size=hparams.batch_size\n",
    "    ),\n",
    "    max_steps=hparams.max_steps,\n",
    "    hooks=None\n",
    ")\n",
    "\n",
    "eval_spec = tf.estimator.EvalSpec(\n",
    "    input_fn = generate_tsv_input_fn(\n",
    "        Params.RAW_EVAL_DATA_FILE_PREFEX+\"*\",\n",
    "        mode=tf.estimator.ModeKeys.EVAL,\n",
    "        num_epochs=1,\n",
    "        batch_size=hparams.batch_size\n",
    "    ),\n",
    "    exporters=[tf.estimator.LatestExporter(\n",
    "        name=\"estimate\", # the name of the folder in which the model will be exported to under export\n",
    "        serving_input_receiver_fn=generate_serving_input_fn(),\n",
    "        exports_to_keep=1,\n",
    "        as_text=False)],\n",
    "    steps=None,\n",
    "    throttle_secs=EVAL_EVERY_SEC\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Run experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import shutil\n",
    "\n",
    "if Params.TRAIN:\n",
    "    if not Params.RESUME_TRAINING:\n",
    "        print(\"Removing previous training artefacts...\")\n",
    "        shutil.rmtree(model_dir, ignore_errors=True)\n",
    "    else:\n",
    "        print(\"Resuming training...\") \n",
    "\n",
    "\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "    time_start = datetime.utcnow() \n",
    "    print(\"Experiment started at {}\".format(time_start.strftime(\"%H:%M:%S\")))\n",
    "    print(\".......................................\") \n",
    "\n",
    "    estimator = create_estimator_hub(hparams, run_config)\n",
    "\n",
    "    tf.estimator.train_and_evaluate(\n",
    "        estimator=estimator,\n",
    "        train_spec=train_spec, \n",
    "        eval_spec=eval_spec\n",
    "    )\n",
    "\n",
    "    time_end = datetime.utcnow() \n",
    "    print(\".......................................\")\n",
    "    print(\"Experiment finished at {}\".format(time_end.strftime(\"%H:%M:%S\")))\n",
    "    print(\"\")\n",
    "    time_elapsed = time_end - time_start\n",
    "    print(\"Experiment elapsed time: {} seconds\".format(time_elapsed.total_seconds()))\n",
    "else:\n",
    "    print \"Training was skipped!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 73124\n",
    "VALID_SIZE = 23079\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "estimator = create_estimator_hub(hparams, run_config)\n",
    "\n",
    "train_metrics = estimator.evaluate(\n",
    "    input_fn = generate_tsv_input_fn(\n",
    "        files_pattern= Params.RAW_TRAIN_DATA_FILE_PREFEX+\"*\", \n",
    "        mode= tf.estimator.ModeKeys.EVAL,\n",
    "        batch_size= TRAIN_SIZE), \n",
    "    steps=1\n",
    ")\n",
    "\n",
    "\n",
    "print(\"############################################################################################\")\n",
    "print(\"# Train Measures: {}\".format(train_metrics))\n",
    "print(\"############################################################################################\")\n",
    "\n",
    "eval_metrics = estimator.evaluate(\n",
    "    input_fn=generate_tsv_input_fn(\n",
    "        files_pattern=Params.RAW_EVAL_DATA_FILE_PREFEX+\"*\", \n",
    "        mode= tf.estimator.ModeKeys.EVAL,\n",
    "        batch_size= TRAIN_SIZE), \n",
    "    steps=1\n",
    ")\n",
    "print(\"\")\n",
    "print(\"############################################################################################\")\n",
    "print(\"# Valid Measures: {}\".format(eval_metrics))\n",
    "print(\"############################################################################################\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Use SavedModel for predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "export_dir = model_dir +\"/export/estimate/\"\n",
    "saved_model_dir = os.path.join(export_dir, os.listdir(export_dir)[0])\n",
    "\n",
    "print(saved_model_dir)\n",
    "print(\"\")\n",
    "\n",
    "predictor_fn = tf.contrib.predictor.from_saved_model(\n",
    "    export_dir = saved_model_dir,\n",
    "    signature_def_key=\"predict\"\n",
    ")\n",
    "\n",
    "output = predictor_fn(\n",
    "    {\n",
    "        'title':[\n",
    "            'Microsoft and Google are joining forces for a new AI framework',\n",
    "            'A new version of Python is mind blowing',\n",
    "            'EU is investigating new data privacy policies'\n",
    "        ]\n",
    "        \n",
    "    }\n",
    ")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
